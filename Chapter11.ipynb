{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNW8f3BPIoi"
      },
      "source": [
        "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQfcyrdyPIok"
      },
      "source": [
        "# Chapter 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA_yhplzPIol"
      },
      "source": [
        "## Additional Setup\n",
        "\n",
        "The repository for the `allennlp` library has been archived. The library was used by `flair` to provide ELMO embeddings. This required PyTorch to be downgraded to version 1.7.1, and it prevented updating other libraries used in this chapter.\n",
        "\n",
        "At this point, the sensible solution is to upgrade `flair` and the other packages to more up-to-date versions and retire the use of ELMO embeddings. For the sake of completion, these embeddings will be replaced by flair embeddings in the code. Some code will be commented out whenever the replacement produces results that are substantially different from those produced by ELMO.\n",
        "\n",
        "Moreover, upgrading the `gensim` library required a few changes in the code since some attributes were changed from its 3.8.3 to its 4.3.3 version. These changes are highlighted in the code.\n",
        "\n",
        "**IMPORTANT**: due to the major version upgrade, the output of several cells will be quite different than those shown in the book. The new Gensim version produces slightly different vocabularies, so you should expect small changes in the number of words in the vocabulary, for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C8BhclloPIom"
      },
      "outputs": [],
      "source": [
        "# # UPDATED\n",
        "# ###########################################################\n",
        "# !pip install gensim==4.3.3\n",
        "# # The library has been archived and won't be used anymore\n",
        "# # # !pip install allennlp==0.9.0\n",
        "# !pip install flair==0.13.1\n",
        "# !pip install torchvision==0.18.1\n",
        "# # # HuggingFace\n",
        "# !pip install transformers==4.42.4\n",
        "# !pip install datasets==2.18.0\n",
        "# ###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5okKRDWPIon",
        "outputId": "1d5bd752-9888-4110-b554-ded8883967d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter11()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter11 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MMmcGqLxPIoo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import errno\n",
        "import requests\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from data_generation.nlp import ALICE_URL, WIZARD_URL, download_text\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in Chapter 10\n",
        "from seq2seq import *\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim  # colab dosen't install gensim lib by default."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16su0aofsi1f",
        "outputId": "3fa64692-b3cb-45b3-80e2-d897f9025d70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fU1AMangTB1o"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim import corpora, downloader\n",
        "from gensim.parsing.preprocessing import *\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L6HR2OpPTtEJ"
      },
      "outputs": [],
      "source": [
        "# !pip install flair==0.13.1\n",
        "from flair.data import Sentence\n",
        "#from flair.embeddings import ELMoEmbeddings, WordEmbeddings, #    TransformerWordEmbeddings, TransformerDocumentEmbeddings\n",
        "from flair.embeddings import WordEmbeddings,     TransformerWordEmbeddings, TransformerDocumentEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sfXyVryWT2mM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "from transformers import (\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BertModel, BertTokenizer, BertForSequenceClassification,\n",
        "    DistilBertModel, DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel, AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, pipeline, TextClassificationPipeline\n",
        ")\n",
        "from transformers.pipelines import SUPPORTED_TASKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOMOKCI3PIop"
      },
      "source": [
        "# Down the Yellow Brick Rabbit Hole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG2bSGFtPIop"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/alice_dorothy.png?raw=1)\n",
        "\n",
        "*Left: \"Alice and the Baby Pig\" illustration by John Tenniel's, from \"Alice's Adventure's in Wonderland\" (1865).*\n",
        "\n",
        "*Right: \"Dorothy meets the Cowardly Lion\" illustration by W.W. Denslow, from \"The Wonderful Wizard of Oz\" (1900)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqGeN0zzPIoq"
      },
      "source": [
        "# Building a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YIiNrb9uPIor"
      },
      "outputs": [],
      "source": [
        "localfolder = 'texts'\n",
        "download_text(ALICE_URL, localfolder)\n",
        "download_text(WIZARD_URL, localfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4IceFMuyPIor"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(localfolder, 'alice28-1476.txt'), 'r') as f:\n",
        "    alice = ''.join(f.readlines()[104:3704])\n",
        "\n",
        "with open(os.path.join(localfolder, 'wizoz10-1740.txt'), 'r') as f:\n",
        "    wizard = ''.join(f.readlines()[310:5100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpw53y4yPIos",
        "outputId": "afaf1ae7-b8e7-43ae-b7f0-dc995ab7dc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                ALICE'S ADVENTURES IN WONDERLAND\n",
            "\n",
            "                          Lewis Carroll\n",
            "\n",
            "               THE MILLENNIUM FULCRUM EDITION 2.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            CHAPTER I\n",
            "\n",
            "                      Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "  Alice was beginning to get very tired of sitting by her sister\n",
            "on the bank, and of having nothing to do:  once or twice she had\n",
            "peeped into the book her sister was reading, but it had no\n",
            "pictures or conversations in it, `and what is the use of a book,'\n",
            "thought Alice `w\n",
            "\n",
            "\n",
            "                    THE WONDERFUL WIZARD OF OZ\n",
            "\n",
            "\n",
            "                          1.  The Cyclone\n",
            "\n",
            "\n",
            "    Dorothy lived in the midst of the great Kansas prairies, with\n",
            "Uncle Henry, who was a farmer, and Aunt Em, who was the farmer's\n",
            "wife.  Their house was small, for the lumber to build it had to be\n",
            "carried by wagon many miles.  There were four walls, a floor and a\n",
            "roof, which made one room; and this room contained a rusty looking\n",
            "cookstove, a cupboard for the dishes, a table, three or four\n",
            "chairs, and th\n"
          ]
        }
      ],
      "source": [
        "print(alice[:500])\n",
        "print('\\n')\n",
        "print(wizard[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JNdQPUzqPIos"
      },
      "outputs": [],
      "source": [
        "text_cfg = \"\"\"fname,start,end\n",
        "alice28-1476.txt,104,3704\n",
        "wizoz10-1740.txt,310,5100\"\"\"\n",
        "bytes_written = open(os.path.join(localfolder, 'lines.cfg'), 'w').write(text_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4tQSexPIot"
      },
      "source": [
        "## Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms_jPDTWPIot",
        "outputId": "208471e6-f302-42ef-be76-be10fc578d4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'following', 'the', 'white', 'rabbit']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sentence = \"I'm following the white rabbit\"\n",
        "tokens = sentence.split(' ')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxbboLMMPIou",
        "outputId": "8b21eecb-6f51-4d9a-eb83-2b5db35379c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1612, 2240)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "corpus_alice = sent_tokenize(alice)\n",
        "corpus_wizard = sent_tokenize(wizard)\n",
        "len(corpus_alice), len(corpus_wizard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ0TkRtRPIou"
      },
      "outputs": [],
      "source": [
        "corpus_alice[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFOXRwwLPIov"
      },
      "outputs": [],
      "source": [
        "corpus_wizard[30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TX0L1NqPIov"
      },
      "outputs": [],
      "source": [
        "def sentence_tokenize(source, quote_char='\\\\', sep_char=',',\n",
        "                      include_header=True, include_source=True,\n",
        "                      extensions=('txt'), **kwargs):\n",
        "    nltk.download('punkt')\n",
        "    # If source is a folder, goes through all files inside it\n",
        "    # that match the desired extensions ('txt' by default)\n",
        "    if os.path.isdir(source):\n",
        "        filenames = [f for f in os.listdir(source)\n",
        "                     if os.path.isfile(os.path.join(source, f)) and\n",
        "                        os.path.splitext(f)[1][1:] in extensions]\n",
        "    elif isinstance(source, str):\n",
        "        filenames = [source]\n",
        "\n",
        "    # If there is a configuration file, builds a dictionary with\n",
        "    # the corresponding start and end lines of each text file\n",
        "    config_file = os.path.join(source, 'lines.cfg')\n",
        "    config = {}\n",
        "    if os.path.exists(config_file):\n",
        "        with open(config_file, 'r') as f:\n",
        "            rows = f.readlines()\n",
        "\n",
        "        for r in rows[1:]:\n",
        "            fname, start, end = r.strip().split(',')\n",
        "            config.update({fname: (int(start), int(end))})\n",
        "\n",
        "    new_fnames = []\n",
        "    # For each file of text\n",
        "    for fname in filenames:\n",
        "        # If there's a start and end line for that file, use it\n",
        "        try:\n",
        "            start, end = config[fname]\n",
        "        except KeyError:\n",
        "            start = None\n",
        "            end = None\n",
        "\n",
        "        # Opens the file, slices the configures lines (if any)\n",
        "        # cleans line breaks and uses the sentence tokenizer\n",
        "        with open(os.path.join(source, fname), 'r') as f:\n",
        "            contents = (''.join(f.readlines()[slice(start, end, None)])\n",
        "                        .replace('\\n', ' ').replace('\\r', ''))\n",
        "        corpus = sent_tokenize(contents, **kwargs)\n",
        "\n",
        "        # Builds a CSV file containing tokenized sentences\n",
        "        base = os.path.splitext(fname)[0]\n",
        "        new_fname = f'{base}.sent.csv'\n",
        "        new_fname = os.path.join(source, new_fname)\n",
        "        with open(new_fname, 'w') as f:\n",
        "            # Header of the file\n",
        "            if include_header:\n",
        "                if include_source:\n",
        "                    f.write('sentence,source\\n')\n",
        "                else:\n",
        "                    f.write('sentence\\n')\n",
        "            # Writes one line for each sentence\n",
        "            for sentence in corpus:\n",
        "                if include_source:\n",
        "                    f.write(f'{quote_char}{sentence}{quote_char}{sep_char}{fname}\\n')\n",
        "                else:\n",
        "                    f.write(f'{quote_char}{sentence}{quote_char}\\n')\n",
        "        new_fnames.append(new_fname)\n",
        "\n",
        "    # Returns list of the newly generated CSV files\n",
        "    return sorted(new_fnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duMi-DzQPIov"
      },
      "outputs": [],
      "source": [
        "new_fnames = sentence_tokenize(localfolder)\n",
        "new_fnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvQ5ikWQPIow"
      },
      "outputs": [],
      "source": [
        "# import spacy\n",
        "# nlp = spacy.blank(\"en\")\n",
        "# nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
        "\n",
        "# sentences = []\n",
        "# for doc in nlp.pipe(corpus_alice):\n",
        "#     sentences.extend(sent.text for sent in doc.sents)\n",
        "\n",
        "# len(sentences), sentences[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNMT7-ClPIow"
      },
      "source": [
        "## HuggingFace's Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5T31ZJtPIow"
      },
      "source": [
        "## Loading a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mmR-5ItPIox"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "\n",
        "dataset = load_dataset(path='csv', data_files=new_fnames, quotechar='\\\\', split=Split.TRAIN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHikb18aPIox"
      },
      "source": [
        "### Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvVIqWAiPIox"
      },
      "outputs": [],
      "source": [
        "dataset.features, dataset.num_columns, dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtH_c0gVPIoy"
      },
      "outputs": [],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDOC9Hh4PIoy"
      },
      "outputs": [],
      "source": [
        "dataset['source'][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-mEXGrPIoy"
      },
      "source": [
        "### Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QprAteUMPIoy"
      },
      "outputs": [],
      "source": [
        "dataset.unique('source')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XbmJVItPIoz"
      },
      "outputs": [],
      "source": [
        "def is_alice_label(row):\n",
        "    is_alice = int(row['source'] == 'alice28-1476.txt')\n",
        "    return {'labels': is_alice}\n",
        "\n",
        "dataset = dataset.map(is_alice_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei4rMiW0PIoz"
      },
      "outputs": [],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL8_4qImPIo0"
      },
      "outputs": [],
      "source": [
        "shuffled_dataset = dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svODbO3NPIo0"
      },
      "outputs": [],
      "source": [
        "split_dataset = shuffled_dataset.train_test_split(test_size=0.2)\n",
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-vhKAPsPIo0"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTeb1APPIo0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v18lfIROPIo1"
      },
      "outputs": [],
      "source": [
        "# !pip install textattack\n",
        "# from textattack.augmentation import EmbeddingAugmenter\n",
        "# augmenter = EmbeddingAugmenter()\n",
        "# feynman = 'What I cannot create, I do not understand.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKSTr__FPIo1"
      },
      "outputs": [],
      "source": [
        "# for i in range(5):\n",
        "#     print(augmenter.augment(feynman))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Apyc3vyPIo1"
      },
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6C3Qto1PIo1"
      },
      "outputs": [],
      "source": [
        "sentence = \"I'm following the white rabbit\"\n",
        "tokens = sentence.split(' ')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMiBbirCPIo2"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import *\n",
        "\n",
        "preprocess_string(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9I0NFCcPIo2"
      },
      "outputs": [],
      "source": [
        "filters = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric]\n",
        "preprocess_string(sentence, filters=filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6zIZlDPPIo2"
      },
      "outputs": [],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "tokens = simple_preprocess(sentence)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S41E3OspPIo3"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj8VJgZ-PIo3"
      },
      "outputs": [],
      "source": [
        "sentences = train_dataset['sentence']\n",
        "tokens = [simple_preprocess(sent) for sent in sentences]\n",
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmGTVyKkPIo3"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbIfKFBIPIo3"
      },
      "outputs": [],
      "source": [
        "dictionary.num_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOPK8ULTPIo4"
      },
      "outputs": [],
      "source": [
        "dictionary.num_pos # processed words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cR7uPunPIo4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dictionary.token2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2R6GIb0PIo5"
      },
      "outputs": [],
      "source": [
        "vocab = list(dictionary.token2id.keys())\n",
        "vocab[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaZdrWZDPIo5"
      },
      "outputs": [],
      "source": [
        "dictionary.cfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_mMaGqPIo6"
      },
      "outputs": [],
      "source": [
        "dictionary.dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8trSPV_PIo6"
      },
      "outputs": [],
      "source": [
        "sentence = 'follow the white rabbit'\n",
        "new_tokens = simple_preprocess(sentence)\n",
        "ids = dictionary.doc2idx(new_tokens)\n",
        "print(new_tokens)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTnw8JNXPIo6"
      },
      "outputs": [],
      "source": [
        "special_tokens = {'[PAD]': 0, '[UNK]': 1}\n",
        "dictionary.patch_with_special_tokens(special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y0NKpAuPIo7"
      },
      "outputs": [],
      "source": [
        "def get_rare_ids(dictionary, min_freq):\n",
        "    rare_ids = [t[0] for t in dictionary.cfs.items() if t[1] < min_freq]\n",
        "    return rare_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO9cRL8fPIo7"
      },
      "outputs": [],
      "source": [
        "def make_vocab(sentences, folder=None, special_tokens=None, vocab_size=None, min_freq=None):\n",
        "    if folder is not None:\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "    # tokenizes the sentences and create a Dictionary\n",
        "    tokens = [simple_preprocess(sent) for sent in sentences]\n",
        "    dictionary = corpora.Dictionary(tokens)\n",
        "    # keeps only the most frequent words (vocab size)\n",
        "    if vocab_size is not None:\n",
        "        dictionary.filter_extremes(keep_n=vocab_size)\n",
        "    # removes rare words (in case the vocab size still\n",
        "    # includes words with low frequency)\n",
        "    if min_freq is not None:\n",
        "        rare_tokens = get_rare_ids(dictionary, min_freq)\n",
        "        dictionary.filter_tokens(bad_ids=rare_tokens)\n",
        "    # gets the whole list of tokens and frequencies\n",
        "    items = dictionary.cfs.items()\n",
        "    # sorts the tokens in descending order\n",
        "    words = [dictionary[t[0]] for t in sorted(dictionary.cfs.items(), key=lambda t: -t[1])]\n",
        "    # prepends special tokens, if any\n",
        "    if special_tokens is not None:\n",
        "        to_add = []\n",
        "        for special_token in special_tokens:\n",
        "            if special_token not in words:\n",
        "                to_add.append(special_token)\n",
        "        words = to_add + words\n",
        "\n",
        "    with open(os.path.join(folder, 'vocab.txt'), 'w') as f:\n",
        "        for word in words:\n",
        "            f.write(f'{word}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x1exiX0PIo7"
      },
      "outputs": [],
      "source": [
        "make_vocab(train_dataset['sentence'], 'our_vocab/', special_tokens=['[PAD]', '[UNK]', '[SEP]', '[CLS]', '[MASK]'], min_freq=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cq5LmV6PIo7"
      },
      "source": [
        "## HugginFace's Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbW7Cd8jPIo8"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer('our_vocab/vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhxCrkJjPIo8"
      },
      "outputs": [],
      "source": [
        "new_sentence = 'follow the white rabbit neo'\n",
        "new_tokens = tokenizer.tokenize(new_sentence)\n",
        "new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6DlCKGxPIo8"
      },
      "outputs": [],
      "source": [
        "new_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06gmmhDvPIo9"
      },
      "outputs": [],
      "source": [
        "new_ids = tokenizer.encode(new_sentence)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98Ccyt_LPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(new_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVOy3y3SPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(new_sentence, add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGBDu8aoPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer(new_sentence, add_special_tokens=False, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8-t1puPIo_"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'follow the white rabbit neo'\n",
        "sentence2 = 'no one can be told what the matrix is'\n",
        "joined_sentences = tokenizer(sentence1, sentence2)\n",
        "joined_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjJ5EQqfPIo_"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(joined_sentences['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNHKY9CrPIo_"
      },
      "outputs": [],
      "source": [
        "separate_sentences = tokenizer([sentence1, sentence2], padding=True)\n",
        "separate_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQDFqKbPIpF"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(separate_sentences['input_ids'][0]))\n",
        "print(separate_sentences['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqsq7GS3PIpG"
      },
      "outputs": [],
      "source": [
        "first_sentences = [sentence1, 'another first sentence']\n",
        "second_sentences = [sentence2, 'a second sentence here']\n",
        "batch_of_pairs = tokenizer(first_sentences, second_sentences)\n",
        "first_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][0])\n",
        "second_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][1])\n",
        "print(first_input)\n",
        "print(second_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U40RGXSPIpG"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(dataset['sentence'],\n",
        "                              padding=True,\n",
        "                              return_tensors='pt',\n",
        "                              max_length=50,\n",
        "                              truncation=True)\n",
        "tokenized_dataset['input_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYIpul-cPIpG"
      },
      "source": [
        "# Before Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U80QzETEPIpH"
      },
      "source": [
        "## One-Hot Encoding (OHE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3mlNgIPIpH"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ohe1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGxP_hdPIpH"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ohe2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2WB4P5PIpH"
      },
      "source": [
        "## Bag of Words (BoW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6ljfjKBPIpH"
      },
      "outputs": [],
      "source": [
        "sentence = 'the white rabbit is a rabbit'\n",
        "bow_tokens = simple_preprocess(sentence)\n",
        "bow_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrldcH-PIpI"
      },
      "outputs": [],
      "source": [
        "bow = dictionary.doc2bow(bow_tokens)\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfbvtiv2PIpI"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank1.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank2.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/fill1.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/fill2.png?raw=1)\n",
        "\n",
        "## N-grams\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ngrams.png?raw=1)\n",
        "\n",
        "## Continuous Bag-of-Words (CBoW)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank_end.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank_center.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDgKYloPIpI"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "## Word2Vec\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/cbow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atT-CpiCPIpI"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embeddings = self.embedding(X)\n",
        "        bow = embeddings.mean(dim=1)\n",
        "        logits = self.linear(bow)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn-7a8LHPIpJ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "dummy_cbow = CBOW(vocab_size=5, embedding_size=3)\n",
        "dummy_cbow.embedding.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PavMubbxPIpJ"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_embed.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ4ZKqOCPIpJ"
      },
      "outputs": [],
      "source": [
        "# tokens: ['is', 'barking']\n",
        "dummy_cbow.embedding(torch.as_tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeo7CI0FPIpK"
      },
      "outputs": [],
      "source": [
        "tiny_vocab = ['the', 'small', 'is', 'barking', 'dog']\n",
        "context_words = ['the', 'small', 'is', 'barking']\n",
        "target_words = ['dog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hplr5JPIpK"
      },
      "outputs": [],
      "source": [
        "batch_context = torch.as_tensor([[0, 1, 2, 3]]).long()\n",
        "batch_target = torch.as_tensor([4]).long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uobj-IG8PIpK"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_cbow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMfauJsZPIpK"
      },
      "outputs": [],
      "source": [
        "cbow_features = dummy_cbow.embedding(batch_context).mean(dim=1)\n",
        "cbow_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19wHR2-PIpK"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_logits.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e5bcIGOPIpL"
      },
      "outputs": [],
      "source": [
        "logits = dummy_cbow.linear(cbow_features)\n",
        "logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_RwBJSPIpL"
      },
      "source": [
        "## What is an Embeddings Anyway?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u0vTklVPIpL"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/rest_discrete.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUBiuMFyPIpL"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/rest_continuous.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPzPn8PaPIpM"
      },
      "outputs": [],
      "source": [
        "ratings = torch.as_tensor([[.7, -.4, .7],\n",
        "                           [.3, .7, -.5],\n",
        "                           [.9, -.55, .8],\n",
        "                           [-.3, .8, .34]]).float()\n",
        "sims = torch.zeros(4, 4)\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        sims[i, j] = F.cosine_similarity(ratings[i], ratings[j], dim=0)\n",
        "sims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkxlGEraPIpM"
      },
      "source": [
        "## Pre-trained Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrvd1NarPIpM"
      },
      "source": [
        "## Global Vectors (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_JFk35PIpM"
      },
      "outputs": [],
      "source": [
        "from gensim import downloader\n",
        "\n",
        "glove = downloader.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# UPDATED\n",
        "###########################################################\n",
        "# The vocab property isn't available anymore at the newer\n",
        "# Gensim version. It was replaced by key_to_index\n",
        "# len(glove.vocab)\n",
        "len(glove.key_to_index)\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDzgsj0bPIpN"
      },
      "outputs": [],
      "source": [
        "glove['alice']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46hbjBCmPIpO"
      },
      "outputs": [],
      "source": [
        "synthetic_queen = glove['king'] - glove['man'] + glove['woman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcrptYwOPIpP"
      },
      "outputs": [],
      "source": [
        "fig = plot_word_vectors(glove,\n",
        "                        ['king', 'man', 'woman', 'synthetic', 'queen'],\n",
        "                        other={'synthetic': synthetic_queen})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtLfGWX7PIpP"
      },
      "outputs": [],
      "source": [
        "glove.similar_by_vector(synthetic_queen, topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEk297ScPIpQ"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/embed_arithmetic.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uv9IVvGPIpQ"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "w_{\\text{king}} - w_{\\text{man}}\\approx w_{\\text{queen}}-w_{\\text{woman}} \\implies w_{\\text{king}} - w_{\\text{man}} + w_{\\text{woman}} \\approx w_{\\text{queen}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7M0z1YMPIpQ"
      },
      "source": [
        "## Using Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuAY6Gk6PIpQ"
      },
      "source": [
        "### Vocabulary Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLS4nhMPIpQ"
      },
      "outputs": [],
      "source": [
        "vocab = list(dictionary.token2id.keys())\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWvfyewTPIpQ"
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# The vocab property isn't available anymore at the newer\n",
        "# Gensim version. It was replaced by key_to_index\n",
        "# unknown_words = sorted(list(set(vocab).difference(set(glove.vocab))))\n",
        "\n",
        "unknown_words = sorted(list(set(vocab).difference(set(glove.key_to_index))))\n",
        "###########################################################\n",
        "print(len(unknown_words))\n",
        "print(unknown_words[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EddC0V1zPIpR"
      },
      "outputs": [],
      "source": [
        "unknown_ids = [dictionary.token2id[w] for w in unknown_words if w not in ['[PAD]', '[UNK]']]\n",
        "unknown_count = np.sum([dictionary.cfs[idx] for idx in unknown_ids])\n",
        "unknown_count, dictionary.num_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWUf3u6CPIpR"
      },
      "outputs": [],
      "source": [
        "def vocab_coverage(gensim_dict, pretrained_wv, special_tokens=('[PAD]', '[UNK]')):\n",
        "    vocab = list(gensim_dict.token2id.keys())\n",
        "    # UPDATED\n",
        "    ###########################################################\n",
        "    # The vocab property isn't available anymore at the newer\n",
        "    # Gensim version. It was replaced by key_to_index\n",
        "    # unknown_words = sorted(list(set(vocab).difference(set(pretrained_wv.vocab))))\n",
        "    unknown_words = sorted(list(set(vocab).difference(set(pretrained_wv.key_to_index))))\n",
        "    ###########################################################\n",
        "    unknown_ids = [gensim_dict.token2id[w] for w in unknown_words if w not in special_tokens]\n",
        "    unknown_count = np.sum([gensim_dict.cfs[idx] for idx in unknown_ids])\n",
        "    cov = 1 - unknown_count / gensim_dict.num_pos\n",
        "    return cov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYZTCStNPIpR"
      },
      "outputs": [],
      "source": [
        "vocab_coverage(dictionary, glove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKzOosVxPIpR"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf8fyy3mPIpS"
      },
      "outputs": [],
      "source": [
        "def make_vocab_from_wv(wv, folder=None, special_tokens=None):\n",
        "    if folder is not None:\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "    # UPDATED\n",
        "    ###########################################################\n",
        "    # The index2word property isn't available anymore at the\n",
        "    # newer Gensim version. It was replaced by index_to_key\n",
        "    # words = wv.index2word\n",
        "    words = wv.index_to_key\n",
        "    ###########################################################\n",
        "    if special_tokens is not None:\n",
        "        to_add = []\n",
        "        for special_token in special_tokens:\n",
        "            if special_token not in words:\n",
        "                to_add.append(special_token)\n",
        "        words = to_add + words\n",
        "\n",
        "    with open(os.path.join(folder, 'vocab.txt'), 'w') as f:\n",
        "        for word in words:\n",
        "            f.write(f'{word}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Y-ub7iPIpS"
      },
      "outputs": [],
      "source": [
        "make_vocab_from_wv(glove, 'glove_vocab/', special_tokens=['[PAD]', '[UNK]'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NIjvZpFPIpS"
      },
      "outputs": [],
      "source": [
        "glove_tokenizer = BertTokenizer('glove_vocab/vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN_vm748PIpS"
      },
      "outputs": [],
      "source": [
        "glove_tokenizer.encode('alice followed the white rabbit', add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwsVqJhtPIpS"
      },
      "outputs": [],
      "source": [
        "len(glove_tokenizer.vocab), len(glove.vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBrqOhP-PIpT"
      },
      "source": [
        "### Special Tokens' Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzAqTQ9CPIpT"
      },
      "outputs": [],
      "source": [
        "special_embeddings = np.zeros((2, glove.vector_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AWIyA3JPIpT"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = np.concatenate([special_embeddings, glove.vectors], axis=0)\n",
        "extended_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMTUfdESPIpT"
      },
      "outputs": [],
      "source": [
        "alice_idx = glove_tokenizer.encode('alice', add_special_tokens=False)\n",
        "np.all(extended_embeddings[alice_idx] == glove['alice'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8_PGFaXPIpU"
      },
      "source": [
        "## Model I - GloVe + Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpeWYcdPIpU"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOc1ZhgdPIpU"
      },
      "outputs": [],
      "source": [
        "train_sentences = train_dataset['sentence']\n",
        "train_labels = train_dataset['labels']\n",
        "\n",
        "test_sentences = test_dataset['sentence']\n",
        "test_labels = test_dataset['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5SLBMioPIpU"
      },
      "outputs": [],
      "source": [
        "train_ids = glove_tokenizer(train_sentences,\n",
        "                            truncation=True,\n",
        "                            padding=True,\n",
        "                            max_length=60,\n",
        "                            add_special_tokens=False,\n",
        "                            return_tensors='pt')['input_ids']\n",
        "train_labels = torch.as_tensor(train_labels).float().view(-1, 1)\n",
        "\n",
        "test_ids = glove_tokenizer(test_sentences,\n",
        "                           truncation=True,\n",
        "                           padding=True,\n",
        "                           max_length=60,\n",
        "                           add_special_tokens=False,\n",
        "                           return_tensors='pt')['input_ids']\n",
        "test_labels = torch.as_tensor(test_labels).float().view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gy9RIDEPIpU"
      },
      "outputs": [],
      "source": [
        "train_tensor_dataset = TensorDataset(train_ids, train_labels)\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_tensor_dataset, batch_size=32, shuffle=True, generator=generator)\n",
        "test_tensor_dataset = TensorDataset(test_ids, test_labels)\n",
        "test_loader = DataLoader(test_tensor_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3CdOpWPIpU"
      },
      "source": [
        "### Pre-Trained PyTorch Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGxrKlyDPIpV"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = torch.as_tensor(extended_embeddings).float()\n",
        "torch_embeddings = nn.Embedding.from_pretrained(extended_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpdJmQNsPIpV"
      },
      "outputs": [],
      "source": [
        "token_ids, labels = next(iter(train_loader))\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwyGjFNSPIpV"
      },
      "outputs": [],
      "source": [
        "token_embeddings = torch_embeddings(token_ids)\n",
        "token_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIHGuQ4WPIpV"
      },
      "outputs": [],
      "source": [
        "token_embeddings.mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT2eZ1EqPIpV"
      },
      "outputs": [],
      "source": [
        "boe_mean = nn.EmbeddingBag.from_pretrained(extended_embeddings, mode='mean')\n",
        "boe_mean(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDyYigyhPIpV"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yaB00G4PIpW"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = torch.as_tensor(extended_embeddings).float()\n",
        "boe_mean = nn.EmbeddingBag.from_pretrained(\n",
        "    extended_embeddings, mode='mean'\n",
        ")\n",
        "torch.manual_seed(41)\n",
        "model = nn.Sequential(\n",
        "    # Embeddings\n",
        "    boe_mean,\n",
        "    # Classifier\n",
        "    nn.Linear(boe_mean.embedding_dim, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1)\n",
        ")\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTWpbPdfPIpW"
      },
      "outputs": [],
      "source": [
        "sbs_emb = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_emb.set_loaders(train_loader, test_loader)\n",
        "sbs_emb.train(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiaVP9uvPIpW"
      },
      "outputs": [],
      "source": [
        "fig = sbs_emb.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCKFSECRPIpW"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_emb.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kqa22QNPIpX"
      },
      "source": [
        "## Model II - GloVe + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH3DdVEFPIpX"
      },
      "outputs": [],
      "source": [
        "class TransfClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, encoder, n_outputs):\n",
        "        super().__init__()\n",
        "        self.d_model = encoder.d_model\n",
        "        self.n_outputs = n_outputs\n",
        "        self.encoder = encoder\n",
        "        self.mlp = nn.Linear(self.d_model, n_outputs)\n",
        "\n",
        "        self.embed = embedding_layer\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_model))\n",
        "\n",
        "    def preprocess(self, X):\n",
        "        # N, L -> N, L, D\n",
        "        src = self.embed(X)\n",
        "        # Special classifier token\n",
        "        # 1, 1, D -> N, 1, D\n",
        "        cls_tokens = self.cls_token.expand(X.size(0), -1, -1)\n",
        "        # Concatenates CLS tokens -> N, 1 + L, D\n",
        "        src = torch.cat((cls_tokens, src), dim=1)\n",
        "        return src\n",
        "\n",
        "    def encode(self, source, source_mask=None):\n",
        "        # Encoder generates \"hidden states\"\n",
        "        states = self.encoder(source, source_mask)\n",
        "        # Gets state from first token only: [CLS]\n",
        "        cls_state = states[:, 0]  # N, 1, D\n",
        "        return cls_state\n",
        "\n",
        "    @staticmethod\n",
        "    def source_mask(X):\n",
        "        cls_mask = torch.ones(X.size(0), 1).type_as(X)\n",
        "        pad_mask = torch.cat((cls_mask, X > 0), dim=1).bool()\n",
        "        return pad_mask.unsqueeze(1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        src = self.preprocess(X)\n",
        "        # Featurizer\n",
        "        cls_state = self.encode(src, self.source_mask(X))\n",
        "        # Classifier\n",
        "        out = self.mlp(cls_state) # N, 1, outputs\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUum393hPIpX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(33)\n",
        "# Loads the pretrained GloVe embeddings into an embedding layer\n",
        "torch_embeddings = nn.Embedding.from_pretrained(extended_embeddings)\n",
        "# Creates a Transformer Encoder\n",
        "layer = EncoderLayer(n_heads=2, d_model=torch_embeddings.embedding_dim, ff_units=128)\n",
        "encoder = EncoderTransf(layer, n_layers=1)\n",
        "# Uses both layers above to build our model\n",
        "model = TransfClassifier(torch_embeddings, encoder, n_outputs=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mBjsLwHPIpX"
      },
      "outputs": [],
      "source": [
        "sbs_transf = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_transf.set_loaders(train_loader, test_loader)\n",
        "sbs_transf.train(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwpEol1PIpY"
      },
      "outputs": [],
      "source": [
        "fig = sbs_transf.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPQvdYWVPIpY"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_transf.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xh8xxgTPIpY"
      },
      "source": [
        "### Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5_67okaPIpY"
      },
      "outputs": [],
      "source": [
        "sentences = ['The white rabbit and Alice ran away', 'The lion met Dorothy on the road']\n",
        "inputs = glove_tokenizer(sentences, add_special_tokens=False, return_tensors='pt')['input_ids']\n",
        "inputs = inputs.to(sbs_transf.device)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu3MXRxgPIpZ"
      },
      "outputs": [],
      "source": [
        "sbs_transf.model.eval()\n",
        "out = sbs_transf.model(inputs)\n",
        "# our model outputs logits, so we turn them into probs\n",
        "torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nld9xIhPIpZ"
      },
      "outputs": [],
      "source": [
        "alphas = sbs_transf.model.encoder.layers[0].self_attn_heads.alphas\n",
        "alphas[:, :, 0, :].squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5gATXCkPIpZ"
      },
      "outputs": [],
      "source": [
        "tokens = [['[CLS]'] + glove_tokenizer.tokenize(sent) for sent in sentences]\n",
        "fig = plot_attention(tokens, alphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNXPXiSlPIpa"
      },
      "source": [
        "# Contextual Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5oqS0x7PIpa"
      },
      "source": [
        "## ELMo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N8dJJJmPIpa"
      },
      "outputs": [],
      "source": [
        "watch1 = \"\"\"\n",
        "The Hatter was the first to break the silence. `What day of the month is it?' he said, turning to Alice:  he had taken his watch out of his pocket, and was looking at it uneasily, shaking it every now and then, and holding it to his ear.\n",
        "\"\"\"\n",
        "\n",
        "watch2 = \"\"\"\n",
        "Alice thought this a very curious thing, and she went nearer to watch them, and just as she came up to them she heard one of them say, `Look out now, Five!  Don't go splashing paint over me like that!\n",
        "\"\"\"\n",
        "\n",
        "sentences = [watch1, watch2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M_qVWdNPIpa"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "flair_sentences = [Sentence(s) for s in sentences]\n",
        "flair_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsasoxvnPIpb"
      },
      "outputs": [],
      "source": [
        "flair_sentences[0].get_token(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdaPu5VNPIpb"
      },
      "outputs": [],
      "source": [
        "flair_sentences[0].tokens[31]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iTjhAuPIpb"
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# Unfortunately, ELMO embeddings were returned by flair\n",
        "# using the allennlp library that has been archived.\n",
        "# from flair.embeddings import ELMoEmbeddings\n",
        "# elmo = ELMoEmbeddings()\n",
        "\n",
        "# As replacement, we'll be using flair's own embeddings instead\n",
        "from flair.embeddings import FlairEmbeddings\n",
        "flair_emb = FlairEmbeddings('news-forward')\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZOGM9xpPIpb"
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# Instead of elmo, embeds sentences using flair embeddings\n",
        "# elmo.embed(flair_sentences)\n",
        "flair_emb.embed(flair_sentences)\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdFVdgRZPIpc"
      },
      "outputs": [],
      "source": [
        "token_watch1 = flair_sentences[0].tokens[31]\n",
        "token_watch2 = flair_sentences[1].tokens[13]\n",
        "token_watch1, token_watch2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XbrpwUfPIpc"
      },
      "outputs": [],
      "source": [
        "token_watch1.embedding, token_watch2.embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucsg8w21PIpd"
      },
      "source": [
        "### Where do ELMo Embeddings come from?\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/elmo_lstm.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/elmo_embed.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AehQ4275PIpd"
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# Unfortunately, this piece of code only makes sense for\n",
        "# ELMO embeddings. Flair embeddings are structured differently\n",
        "# and therefore this code is commented out.\n",
        "# token_watch1.embedding[0], token_watch1.embedding[512]\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFAgysccPIpd"
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# Unfortunately, this piece of code only makes sense for\n",
        "# ELMO embeddings. Flair embeddings are structured differently\n",
        "# and therefore this code is commented out.\n",
        "# (token_watch1.embedding[:1024] == token_watch2.embedding[:1024]).all()\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hxX2iMHPIpd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "similarity(token_watch1.embedding, token_watch2.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF_w6p-rPIpe"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(embeddings, sentence):\n",
        "    sent = Sentence(sentence)\n",
        "    embeddings.embed(sent)\n",
        "    return torch.stack([token.embedding for token in sent.tokens]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEU5nynEPIpe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "# Instead of elmo, embeds sentences using flair embeddings\n",
        "# get_embeddings(elmo, watch1)\n",
        "get_embeddings(flair_emb, watch1)\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_xJposPIpe"
      },
      "source": [
        "## GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Y54X9_PIpe"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "glove_embedding = WordEmbeddings('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mibBx6mvPIpe"
      },
      "outputs": [],
      "source": [
        "new_flair_sentences = [Sentence(s) for s in sentences]\n",
        "glove_embedding.embed(new_flair_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzaqGrPBPIpf"
      },
      "outputs": [],
      "source": [
        "torch.all(new_flair_sentences[0].tokens[31].embedding == new_flair_sentences[1].tokens[13].embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIH-ZF35PIpf"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wppmv3qLPIpf"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "bert_flair = TransformerWordEmbeddings('bert-base-uncased', layers='-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXKlNCfdPIpf"
      },
      "outputs": [],
      "source": [
        "embed1 = get_embeddings(bert_flair, watch1)\n",
        "embed2 = get_embeddings(bert_flair, watch2)\n",
        "embed2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNZPqZPUPIpg"
      },
      "outputs": [],
      "source": [
        "bert_watch1 = embed1[31]\n",
        "bert_watch2 = embed2[13]\n",
        "bert_watch1, bert_watch2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvTJj1vcPIpg"
      },
      "outputs": [],
      "source": [
        "similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "similarity(bert_watch1, bert_watch2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt9Wl8aIPIpg"
      },
      "source": [
        "## Document Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgU-haTVPIpg"
      },
      "outputs": [],
      "source": [
        "documents = [Sentence(watch1), Sentence(watch2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLbRmOR_PIpg"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "bert_doc = TransformerDocumentEmbeddings('bert-base-uncased')\n",
        "bert_doc.embed(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB6GBlxYPIpg"
      },
      "outputs": [],
      "source": [
        "documents[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX4ZkKaPPIph"
      },
      "outputs": [],
      "source": [
        "documents[0].tokens[31].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8XT_0ahPIph"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(embeddings, sentence):\n",
        "    sent = Sentence(sentence)\n",
        "    embeddings.embed(sent)\n",
        "    if len(sent.embedding):\n",
        "        return sent.embedding.float()\n",
        "    else:\n",
        "        return torch.stack([token.embedding for token in sent.tokens]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yt6esWrPIph"
      },
      "outputs": [],
      "source": [
        "get_embeddings(bert_doc, watch1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp3LpDtwPIph"
      },
      "source": [
        "## Model III - Preprocessing Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xafyHRinPIpi"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62H2IcGYPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc = train_dataset.map(lambda row: {'embeddings': get_embeddings(bert_doc, row['sentence'])})\n",
        "test_dataset_doc = test_dataset.map(lambda row: {'embeddings': get_embeddings(bert_doc, row['sentence'])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5rURvw6PIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc.set_format(type='torch', columns=['embeddings', 'labels'])\n",
        "test_dataset_doc.set_format(type='torch', columns=['embeddings', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Ov7ucKPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc['embeddings']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfMQkFCHPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc = TensorDataset(train_dataset_doc['embeddings'].float(),\n",
        "                                  train_dataset_doc['labels'].view(-1, 1).float())\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_dataset_doc, batch_size=32, shuffle=True, generator=generator)\n",
        "\n",
        "test_dataset_doc = TensorDataset(test_dataset_doc['embeddings'].float(),\n",
        "                                 test_dataset_doc['labels'].view(-1, 1).float())\n",
        "test_loader = DataLoader(test_dataset_doc, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDOr-sgLPIpi"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEjwk7i8PIpj"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(41)\n",
        "model = nn.Sequential(\n",
        "    # Classifier\n",
        "    nn.Linear(bert_doc.embedding_length, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3, 1)\n",
        ")\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93pg9nrOPIpj"
      },
      "outputs": [],
      "source": [
        "sbs_doc_emb = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_doc_emb.set_loaders(train_loader, test_loader)\n",
        "sbs_doc_emb.train(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jSTsMaSPIpj"
      },
      "outputs": [],
      "source": [
        "fig = sbs_doc_emb.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZpLkF80PIpj"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_doc_emb.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLKZdrC9PIpj"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgTF0CIPIpk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "auto_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "print(auto_model.__class__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYjuuJG-PIpk"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpDTWGNBPIpk"
      },
      "outputs": [],
      "source": [
        "bert_model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBkvBjp5PIpk"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7ZcsRmdPIpk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "len(bert_tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyF47gBWPIpk"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'Alice is inexplicably following the white rabbit'\n",
        "sentence2 = 'Follow the white rabbit, Neo'\n",
        "tokens = bert_tokenizer(sentence1, sentence2, return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brcF_Ox1PIpl"
      },
      "outputs": [],
      "source": [
        "print(bert_tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cjrk1qCPIpl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(auto_tokenizer.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0A5jpZJPIpl"
      },
      "source": [
        "## Input Embeddings\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_input_embed.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOhc49UmPIpl"
      },
      "outputs": [],
      "source": [
        "input_embeddings = bert_model.embeddings\n",
        "input_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo5g1KN2PIpm"
      },
      "outputs": [],
      "source": [
        "token_embeddings = input_embeddings.word_embeddings\n",
        "token_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr_sA4KEPIpm"
      },
      "outputs": [],
      "source": [
        "input_token_emb = token_embeddings(tokens['input_ids'])\n",
        "input_token_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx5LiAACPIpm"
      },
      "outputs": [],
      "source": [
        "position_embeddings = input_embeddings.position_embeddings\n",
        "position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6e0kmEnPIpm"
      },
      "outputs": [],
      "source": [
        "position_ids = torch.arange(512).expand((1, -1))\n",
        "position_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqZUNJBhPIpn"
      },
      "outputs": [],
      "source": [
        "seq_length = tokens['input_ids'].size(1)\n",
        "input_pos_emb = position_embeddings(position_ids[:, :seq_length])\n",
        "input_pos_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DXX4yiEPIpn"
      },
      "outputs": [],
      "source": [
        "segment_embeddings = input_embeddings.token_type_embeddings\n",
        "segment_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgCRoV2sPIpn"
      },
      "outputs": [],
      "source": [
        "input_seg_emb = segment_embeddings(tokens['token_type_ids'])\n",
        "input_seg_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUkjZrKTPIpn"
      },
      "outputs": [],
      "source": [
        "input_emb = input_token_emb + input_pos_emb + input_seg_emb\n",
        "input_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGkuZ7UdPIpn"
      },
      "source": [
        "## Pretraining Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2FoG2AGPIpn"
      },
      "source": [
        "### Masked Language Model (MLM)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_mlm.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTw0eDKePIpo"
      },
      "outputs": [],
      "source": [
        "sentence = 'Alice is inexplicably following the white rabbit'\n",
        "tokens = bert_tokenizer(sentence)\n",
        "tokens['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yCbwZXgPIpo"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "torch.manual_seed(41)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm_probability=0.15)\n",
        "mlm_tokens = data_collator([tokens])\n",
        "mlm_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI180xPwPIpo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(bert_tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "print(bert_tokenizer.convert_ids_to_tokens(mlm_tokens['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyTICtkbPIpo"
      },
      "source": [
        "### Next Sentence Prediction (NSP)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_nsp.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWrQN9DpPIpo"
      },
      "outputs": [],
      "source": [
        "bert_model.pooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVOtEwePIpp"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'alice follows the white rabbit'\n",
        "sentence2 = 'follow the white rabbit neo'\n",
        "bert_tokenizer(sentence1, sentence2, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HkUpyLUPIpp"
      },
      "source": [
        "## Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C-AaHkFPIpp"
      },
      "outputs": [],
      "source": [
        "sentence = train_dataset[0]['sentence']\n",
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmqPWjJuPIpp"
      },
      "outputs": [],
      "source": [
        "tokens = bert_tokenizer(sentence,\n",
        "                        padding='max_length',\n",
        "                        max_length=30,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ssmrA7PIpp"
      },
      "outputs": [],
      "source": [
        "bert_model.eval()\n",
        "out = bert_model(input_ids=tokens['input_ids'],\n",
        "                 attention_mask=tokens['attention_mask'],\n",
        "                 output_attentions=True,\n",
        "                 output_hidden_states=True,\n",
        "                 return_dict=True)\n",
        "out.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0lNYShtPIpq"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_embeddings.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9qGa0qaPIpq"
      },
      "outputs": [],
      "source": [
        "last_hidden_batch = out['last_hidden_state']\n",
        "last_hidden_sentence = last_hidden_batch[0]\n",
        "# Removes hidden states for [PAD] tokens using the mask\n",
        "mask = tokens['attention_mask'].squeeze().bool()\n",
        "embeddings = last_hidden_sentence[mask]\n",
        "# Removes embeddings for the first [CLS] and last [SEP] tokens\n",
        "embeddings[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlqC7lDSPIpq"
      },
      "outputs": [],
      "source": [
        "get_embeddings(bert_flair, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNKAkRLyPIpq"
      },
      "outputs": [],
      "source": [
        "print(len(out['hidden_states']))\n",
        "print(out['hidden_states'][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451QgYohPIpq"
      },
      "outputs": [],
      "source": [
        "(out['hidden_states'][0] == bert_model.embeddings(tokens['input_ids'])).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93o1_w-EPIpr"
      },
      "outputs": [],
      "source": [
        "(out['hidden_states'][-1] == out['last_hidden_state']).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYA0p0XqPIpr"
      },
      "outputs": [],
      "source": [
        "(out['pooler_output'] == bert_model.pooler(out['last_hidden_state'])).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsdP4CnSPIpr"
      },
      "outputs": [],
      "source": [
        "print(len(out['attentions']))\n",
        "print(out['attentions'][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlX9GImRPIpr"
      },
      "source": [
        "## Model IV - Classifying using BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWB0IsJEPIpr"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, ff_units, n_outputs, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.d_model = bert_model.config.dim\n",
        "        self.n_outputs = n_outputs\n",
        "        self.encoder = bert_model\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_units, n_outputs)\n",
        "        )\n",
        "\n",
        "    def encode(self, source, source_mask=None):\n",
        "        states = self.encoder(input_ids=source,\n",
        "                              attention_mask=source_mask)[0]\n",
        "        cls_state = states[:, 0]\n",
        "        return cls_state\n",
        "\n",
        "    def forward(self, X):\n",
        "        source_mask = (X > 0)\n",
        "        # Featurizer\n",
        "        cls_state = self.encode(X, source_mask)\n",
        "        # Classifier\n",
        "        out = self.mlp(cls_state)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN86j0y9PIps"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLFm2rAtPIps"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(hf_dataset, sentence_field, label_field, tokenizer, **kwargs):\n",
        "    sentences = hf_dataset[sentence_field]\n",
        "    token_ids = tokenizer(sentences, return_tensors='pt', **kwargs)['input_ids']\n",
        "    labels = torch.as_tensor(hf_dataset[label_field])\n",
        "    dataset = TensorDataset(token_ids, labels)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvCj__6mPIps"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer_kwargs = dict(truncation=True, padding=True, max_length=30, add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRuvrZRfPIps"
      },
      "outputs": [],
      "source": [
        "train_dataset_float = train_dataset.map(lambda row: {'labels': [float(row['labels'])]})\n",
        "test_dataset_float = test_dataset.map(lambda row: {'labels': [float(row['labels'])]})\n",
        "\n",
        "train_tensor_dataset = tokenize_dataset(train_dataset_float, 'sentence', 'labels', auto_tokenizer, **tokenizer_kwargs)\n",
        "test_tensor_dataset = tokenize_dataset(test_dataset_float, 'sentence', 'labels', auto_tokenizer, **tokenizer_kwargs)\n",
        "\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_tensor_dataset, batch_size=4, shuffle=True, generator=generator)\n",
        "test_loader = DataLoader(test_tensor_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zRWx9gPIps"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z1oEHNkPIps"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(41)\n",
        "bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = BERTClassifier(bert_model, 128, n_outputs=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZL4UQsVPIpt"
      },
      "outputs": [],
      "source": [
        "sbs_bert = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_bert.set_loaders(train_loader, test_loader)\n",
        "sbs_bert.train(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uma227MPIpt"
      },
      "outputs": [],
      "source": [
        "sbs_bert.count_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOE-3A4jPIpt"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_bert.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-YD9bvDPIpt"
      },
      "source": [
        "# Fine-Tuning with HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXihCcZSPIpt"
      },
      "source": [
        "## Sequence Classification (or Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsEGVD3wPIpt"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "torch.manual_seed(42)\n",
        "bert_cls = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7MQ1ApTPIpu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "auto_cls = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "print(auto_cls.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3eWB1c2PIpu"
      },
      "source": [
        "## Tokenized Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmJOKQcmPIpu"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "def tokenize(row):\n",
        "    return auto_tokenizer(row['sentence'],\n",
        "                          truncation=True,\n",
        "                          padding='max_length',\n",
        "                          max_length=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzW2xAeDPIpu"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCToy9BpPIpv"
      },
      "outputs": [],
      "source": [
        "print(tokenized_train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJGHlBQ9PIpv"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQJ3GjfPIpv"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWBcmaN1PIpv"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHy5s29tPIpv"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(model=bert_cls, train_dataset=tokenized_train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pDhbwdPIpw"
      },
      "outputs": [],
      "source": [
        "trainer.args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMVuTOSUPIpw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='output',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=300,\n",
        "    logging_steps=300,\n",
        "    gradient_accumulation_steps=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFxIhoybPIpw"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKo42k7RPIpw"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=bert_cls,\n",
        "                  args=training_args,\n",
        "                  train_dataset=tokenized_train_dataset,\n",
        "                  eval_dataset=tokenized_test_dataset,\n",
        "                  compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wdDiBP4PIpw"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_i6kvF5PIpx"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KOMV6iQPIpx"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('bert_alice_vs_wizard')\n",
        "os.listdir('bert_alice_vs_wizard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WpIt7WvPIpx"
      },
      "outputs": [],
      "source": [
        "loaded_model = AutoModelForSequenceClassification.from_pretrained('bert_alice_vs_wizard')\n",
        "loaded_model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrmSH5kSPIpx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "loaded_model.to(device)\n",
        "loaded_model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_6MnEPTPIpx"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_dt7C28PIpy"
      },
      "outputs": [],
      "source": [
        "sentence = 'Down the yellow brick rabbit hole'\n",
        "tokens = auto_tokenizer(sentence, return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAJtVv5fPIpy"
      },
      "outputs": [],
      "source": [
        "print(type(tokens))\n",
        "tokens.to(loaded_model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs4-3g2DPIpy"
      },
      "outputs": [],
      "source": [
        "loaded_model.eval()\n",
        "logits = loaded_model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_81JJd68PIpy"
      },
      "outputs": [],
      "source": [
        "logits.logits.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOIE2XY-PIpy"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTdPgNWAPIpy"
      },
      "outputs": [],
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "device_index = loaded_model.device.index if loaded_model.device.type != 'cpu' else -1\n",
        "classifier = TextClassificationPipeline(model=loaded_model,\n",
        "                                        tokenizer=auto_tokenizer,\n",
        "                                        device=device_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkhu6QaOPIpz"
      },
      "outputs": [],
      "source": [
        "classifier(['Down the Yellow Brick Rabbit Hole', 'Alice rules!'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57PmBUrZPIpz"
      },
      "outputs": [],
      "source": [
        "loaded_model.config.id2label = {0: 'Wizard', 1: 'Alice'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xsmzplHPIpz"
      },
      "outputs": [],
      "source": [
        "classifier(['Down the Yellow Brick Rabbit Hole', 'Alice rules!'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPv9QGnPPIpz"
      },
      "source": [
        "## More Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orvV3n9wPIpz"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGiwPvGfPIp0"
      },
      "outputs": [],
      "source": [
        "sentence = train_dataset[0]['sentence']\n",
        "print(sentence)\n",
        "print(sentiment(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x456QWjEPIp0"
      },
      "outputs": [],
      "source": [
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "# UPDATED\n",
        "###########################################################\n",
        "# sentiment-analysis was replaced by text-classification\n",
        "# in the dictionary of supported tasks\n",
        "# SUPPORTED_TASKS['sentiment-analysis']\n",
        "SUPPORTED_TASKS['text-classification']\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk3TWVXxPIp0"
      },
      "outputs": [],
      "source": [
        "SUPPORTED_TASKS['text-generation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpfW8LWvPIp0"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gs2m5QtPIp0"
      },
      "outputs": [],
      "source": [
        "text_generator = pipeline(\"text-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNIhhyloPIp1"
      },
      "outputs": [],
      "source": [
        "text_generator.model.config.task_specific_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAlA7qvJPIp1"
      },
      "outputs": [],
      "source": [
        "base_text = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do:  once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,'thought Alice `without pictures or conversation?' So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGsxW7xVPIp1"
      },
      "outputs": [],
      "source": [
        "result = text_generator(base_text, max_length=250)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb1uve8LPIp1"
      },
      "source": [
        "# Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBjmamx_PIp1"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKf7qtJkPIp1"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(path='csv', data_files=['texts/alice28-1476.sent.csv'], quotechar='\\\\', split=Split.TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRxp47A-PIp2"
      },
      "outputs": [],
      "source": [
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVF5IzXsPIp2"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "def tokenize(row):\n",
        "    return auto_tokenizer(row['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUBOlXsqPIp2"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-4cJ4pAPIp2"
      },
      "outputs": [],
      "source": [
        "list(map(len, tokenized_train_dataset[0:6]['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWqqWJ3PIp2"
      },
      "source": [
        "### \"Packed\" Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7aJwvXsPIp3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/block_tokens.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lAMC3OtPIp3"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py\n",
        "def group_texts(examples, block_size=128):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TirMA67-PIp3"
      },
      "outputs": [],
      "source": [
        "lm_train_dataset = tokenized_train_dataset.map(group_texts, batched=True)\n",
        "lm_test_dataset = tokenized_test_dataset.map(group_texts, batched=True)\n",
        "lm_train_dataset.set_format(type='torch')\n",
        "lm_test_dataset.set_format(type='torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwOa4nluPIp3"
      },
      "outputs": [],
      "source": [
        "print(lm_train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkJM5tc3PIp3"
      },
      "outputs": [],
      "source": [
        "len(lm_train_dataset), len(lm_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLxoOb-PIp3"
      },
      "source": [
        "## Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9NLtw1APIp4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print(model.__class__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fI5PE_nPIp4"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(auto_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta_7BbrVPIp4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='output',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    logging_steps=50,\n",
        "    gradient_accumulation_steps=4,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=lm_train_dataset,\n",
        "                  eval_dataset=lm_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WQN-YhePIp4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ru9UY-NPIp5"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYsl0aOiPIp5"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhkK5TIFPIp5"
      },
      "outputs": [],
      "source": [
        "device_index = model.device.index if model.device.type != 'cpu' else -1\n",
        "gpt2_gen = pipeline('text-generation', model=model, tokenizer=auto_tokenizer, device=device_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF0RfSp4PIp5"
      },
      "outputs": [],
      "source": [
        "result = gpt2_gen(base_text, max_length=250)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQG_yBYhPIp6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}